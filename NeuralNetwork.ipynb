{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd60c4fVdLtU",
        "colab_type": "text"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pCVh0z1dHSy",
        "colab_type": "code",
        "outputId": "a5547fc1-8a7e-4a30-8668-1b9d747b3f09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Libraries for pre-processing\n",
        "import seaborn as sns\n",
        "import numpy.random as rand\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from random import sample\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDGvsg-_dP3S",
        "colab_type": "text"
      },
      "source": [
        "**Pre-Processing Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWet5wLbfB7T",
        "colab_type": "text"
      },
      "source": [
        "*Patients*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkTyq4dudVFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Break into Per Patient DataFrames'''\n",
        "def patients(df):\n",
        "    p = {}\n",
        "    for i in df['Subject ID'].unique():\n",
        "        p[i-1] = df.loc[df['Subject ID'] == i]\n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiJtoXdgfYlC",
        "colab_type": "text"
      },
      "source": [
        "*Summarize Patients*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRfoHIcSfWyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Get Feature Features - mean + std'''\n",
        "def stats(df):\n",
        "    # initialize features\n",
        "    features = pd.DataFrame()\n",
        "    # for each column in DataFrame\n",
        "    for c in df.columns:\n",
        "        # create a new feature of its mean\n",
        "        features[c + ' mean'] = [df[c].mean(axis=0)]\n",
        "        # create a new feature of its std\n",
        "        features[c + ' std'] = [df[c].std(axis=0)]\n",
        "    # return features\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eftTjVIVfaL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Get Stats based Patients'''\n",
        "def stats_patients(df, pat_func=patients):\n",
        "    # get patients\n",
        "    p = pat_func(df)\n",
        "    # intialize stat based patients dictionary\n",
        "    s = {}\n",
        "    # for each patient\n",
        "    for (k,v) in p.items():\n",
        "        s[k] = stats(v).drop(['Subject ID mean', 'UPDRS mean', 'class info mean', \n",
        "                              'Subject ID std', 'UPDRS std', 'class info std',], axis=1)\n",
        "        s[k]['Subject ID'] = v['Subject ID'].values[0]\n",
        "        s[k]['UPDRS'] = v['UPDRS'].values[0]\n",
        "        s[k]['class info'] = v['class info'].values[0]\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJYGbaqJfhnP",
        "colab_type": "text"
      },
      "source": [
        "*Normalize Patients*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxy8U_1RfkI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Normalize the features of each patient'''\n",
        "def normalize_patients(df):\n",
        "    # remove labels and ID\n",
        "    data = df.drop(['Subject ID', 'UPDRS', 'class info'], axis=1)\n",
        "    # create Scaler\n",
        "    scale = StandardScaler()\n",
        "    # fit and transfrom the data\n",
        "    normalized = pd.DataFrame(scale.fit_transform(data), columns=names[1:])\n",
        "    # put labels and ID back in\n",
        "    normalized['Subject ID'] = df['Subject ID']\n",
        "    normalized['UPDRS']      = df['UPDRS']\n",
        "    normalized['class info'] = df['class info']\n",
        "    \n",
        "    # break into patients and return\n",
        "    return patients(normalized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta6XDsyffn1C",
        "colab_type": "text"
      },
      "source": [
        "Normalize & Summarize Patients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqMl1cikfp1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Normalized and Stats based for each patient'''\n",
        "def stats_norm_patients(df):\n",
        "    return stats_patients(df, pat_func=normalize_patients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqx0XNVJ2i8e",
        "colab_type": "text"
      },
      "source": [
        "**Splitting Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtPM9DFD2imC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def partition(patients, size):\n",
        "#  print(\"entered function\")\n",
        "\n",
        "\n",
        "  # Initializing Neural Network\n",
        "  classifier = Sequential()\n",
        "  # Adding the input layer and the first hidden layer\n",
        "  # input_dim = 26 or 52 features\n",
        "  # (input_dim + 1)/2 ~ 14 or 27 neurons\n",
        "  classifier.add(Dense(14, input_dim = size, activation = 'relu'))\n",
        "  # Adding the second hidden layer\n",
        "  classifier.add(Dense(14, activation = 'relu'))\n",
        "  classifier.add(Dense(14, activation = 'relu'))\n",
        "  classifier.add(Dense(14, activation = 'relu'))\n",
        "  classifier.add(Dense(14, activation = 'relu'))\n",
        "  classifier.add(Dense(14, activation = 'relu'))\n",
        "  classifier.add(Dense(14, activation = 'relu'))\n",
        "  # Adding the output layer\n",
        "  classifier.add(Dense(1, activation = 'sigmoid'))\n",
        "  # Compiling the ANN\n",
        "  classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "  # correct terms\n",
        "  tp = 0 # true +\n",
        "  tn = 0 # true -\n",
        "  fp = 0 # false +\n",
        "  fn = 0 # false -\n",
        "\n",
        "  # Partition ratio\n",
        "  ratio = 0.2\n",
        "  test_size = ratio*40\n",
        "  train_size = (1-ratio)*40\n",
        "\n",
        "  ############### Partitioning patient dataset ###############\n",
        "\n",
        "  # get labels\n",
        "  df_train = patients[0]\n",
        "  for i in range(1,39):\n",
        "    df_train = df_train.append(patients[i])\n",
        "\n",
        "  labels = df_train['class info'].values\n",
        "#  print(\"labels complete\")    \n",
        "  # partition test/train datasets\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(df_train, labels, test_size=ratio, random_state=42)\n",
        "  \n",
        "  X_train = X_train.drop(['Subject ID', 'UPDRS', 'class info'], axis=1)\n",
        "  X_test  = X_test.drop(['Subject ID', 'UPDRS', 'class info'], axis=1)\n",
        "\n",
        " # print(\"partition complete\")\n",
        " # print(\"X_train: \" + str(X_train.shape))\n",
        " # print(\"Y_train: \" + str(Y_train.shape))\n",
        " # print(\"X_test: \" + str(X_test.shape))\n",
        " # print(\"Y_test: \" + str(Y_test.shape))\n",
        "\n",
        "  # Fitting NN to the Training Set\n",
        "  classifier.fit(X_train, Y_train, batch_size = 10, epochs = 150, verbose = 0)\n",
        "  print(\"fitting complete\")\n",
        "  \n",
        "  # predict on test\n",
        "  prediction = np.where(classifier.predict(X_test) > 0.5, 1, 0)\n",
        "  # flag the correct entries\n",
        "  correct = np.where(prediction == Y_test, 1, 0)\n",
        "  # get the accuracy result\n",
        "  result = np.rint(correct.mean())\n",
        "  \n",
        "  accuracy = accuracy_score(Y_test, prediction)\n",
        "\n",
        "  # Correct\n",
        "  tp += result * Y_test\n",
        "  tn += result * (1-Y_test)\n",
        "  fp += (1-result) * (1-Y_test)\n",
        "  fn += (1-result) * Y_test\n",
        "\n",
        "  # return the accuracy\n",
        "  #a = ((tp+tn) / (tp+tn+fp+fn))\n",
        "  print(\"Accuracy: \" + str(accuracy))\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4vJfDQLeDW_",
        "colab_type": "text"
      },
      "source": [
        "**Leave One Out**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5pD2QuCeEDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Leave one out'''\n",
        "# given the input of a set of dataframes, [patients], representing each patient\n",
        "#  and a classifier, [classifier], run leave one out\n",
        "def leave_one_out(patients, classifier):\n",
        "    \n",
        "    # create LOO splitter\n",
        "    loo = LeaveOneOut()\n",
        "    # create patient based splits\n",
        "    splits = loo.split(patients)\n",
        "\n",
        "    # correct terms\n",
        "    tp = 0 # true +\n",
        "    tn = 0 # true -\n",
        "    fp = 0 # false +\n",
        "    fn = 0 # false -\n",
        "   \n",
        "    # for each split\n",
        "    for train_index, test_index in splits:\n",
        "                    \n",
        "        # get testing data\n",
        "        df_test = patients[test_index[0]]\n",
        "        \n",
        "        # get training data - combining them all in patients\n",
        "        df_train = patients[train_index[0]]\n",
        "        for i in train_index[1:-1]:\n",
        "            df_train = df_train.append(patients[i])\n",
        "            \n",
        "        # Get examples\n",
        "        X_train = df_train.drop(['Subject ID', 'UPDRS', 'class info'], axis=1)\n",
        "        X_test  = df_test.drop(['Subject ID', 'UPDRS', 'class info'], axis=1)\n",
        "        # Get labels\n",
        "        Y_train = df_train['class info'].values\n",
        "        Y_test  = df_test['class info'].values\n",
        "\n",
        "        ##################### Creating Neural Net #####################\n",
        "         \n",
        "       \n",
        "        # Fitting NN to the Training Set\n",
        "        classifier.fit(X_train, Y_train, batch_size = 10, epochs = 150, verbose = 0)\n",
        "\n",
        "        print(\"here\")\n",
        "        \n",
        "        # predict on test\n",
        "        prediction = np.where(classifier.predict(X_test) > 0.5, 1, 0)\n",
        "        # flag the correct entries\n",
        "        correct = np.where(prediction == Y_test, 1, 0)\n",
        "        # get the accuracy result\n",
        "        result = np.rint(correct.mean())\n",
        "        \n",
        "    \n",
        "        # Correct\n",
        "        tp += result * Y_test\n",
        "        tn += result * (1-Y_test)\n",
        "        fp += (1-result) * (1-Y_test)\n",
        "        fn += (1-result) * Y_test\n",
        "        \n",
        "    # return the accuracy\n",
        "    a = ((tp+tn) / loo.get_n_splits(patients))[0]\n",
        "    print(\"accuracy = \" + str(a))\n",
        "    return a\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq6lSwn7djX4",
        "colab_type": "text"
      },
      "source": [
        "**Importing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZKeQyVhdiUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Training Data\n",
        "# column names\n",
        "names = ['Subject ID',\n",
        "            'Jitter (local)', 'Jitter (local, abs)', 'Jitter (rap)', \n",
        "            'Jitter (ppq5)', 'Jitter (ddp)', 'Shimmer (local)', \n",
        "            'Shimmer (local, dB)', 'Shimmer (apq3)', 'Shimmer (apq5)', \n",
        "            'Shimmer (apq11)', 'Shimmer (dda)', 'AC', 'NTH', 'HTN',\n",
        "            'Median Pitch', 'Mean Pitch', 'Std Dev Pitch', 'Min Pitch', \n",
        "            'Max Pitch', 'Num Pulses', 'Num Periods', 'Mean Period',\n",
        "            'Std Dev Periods', 'Frac Unvoiced Frames', 'Num  Breaks',\n",
        "            'Degree of Breaks']\n",
        "# training column names\n",
        "train_names = names + ['UPDRS', 'class info']\n",
        "               \n",
        "df = pd.read_csv(\"train_data.txt\", \n",
        "                       header=None,\n",
        "                       names =train_names)\n",
        "#df.sample(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXd52ruOWXrp",
        "colab_type": "text"
      },
      "source": [
        "**Varying Preprocessing Method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttsAImUhgFEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing functions\n",
        "\n",
        "P = [patients, stats_patients, normalize_patients, stats_norm_patients]\n",
        "features = [26, 52, 26, 52]\n",
        "neurons = [14, 27, 14, 27]\n",
        "#P = [patients]\n",
        "\n",
        "# results\n",
        "results = np.zeros(len(P))\n",
        "\n",
        "# calculate them all\n",
        "for i in range(len(P)):\n",
        "    # not use LOO\n",
        "    # results[i] = partition(P[i](df), features[i])\n",
        "    # Initializing Neural Network\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(neurons[i], input_dim = features[i], activation = 'relu'))\n",
        "    classifier.add(Dense(neurons[i], activation = 'relu'))\n",
        "    classifier.add(Dense(1, activation = 'sigmoid'))\n",
        "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    # use LOO\n",
        "    results[i] = leave_one_out(P[i](df), classifier)\n",
        "\n",
        "print(\"results = \" + str(results))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsqtf1UBneeR",
        "colab_type": "text"
      },
      "source": [
        "**Varying # Layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usZZGrdqnYYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "P = [stats_norm_patients]\n",
        "features = 52\n",
        "neurons = 27\n",
        "#P = [patients]\n",
        "\n",
        "# results\n",
        "result3 = 0\n",
        "result5 = 0\n",
        "\n",
        "\n",
        "# Initializing Neural Network N = 3\n",
        "c3 = Sequential()\n",
        "c3.add(Dense(neurons, input_dim = features, activation = 'relu'))\n",
        "c3.add(Dense(neurons, activation = 'relu'))\n",
        "c3.add(Dense(neurons, activation = 'relu'))\n",
        "c3.add(Dense(1, activation = 'sigmoid'))\n",
        "c3.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "result3 = leave_one_out(P[0](df), c3)\n",
        "print(\"N = 3 --> Accuracy = \" + str(result3))\n",
        "\n",
        "# Initializing Neural Network N = 5\n",
        "c5 = Sequential()\n",
        "c5.add(Dense(neurons, input_dim = features, activation = 'relu'))\n",
        "c5.add(Dense(neurons, activation = 'relu'))\n",
        "c5.add(Dense(neurons, activation = 'relu'))\n",
        "c5.add(Dense(neurons, activation = 'relu'))\n",
        "c5.add(Dense(neurons, activation = 'relu'))\n",
        "c5.add(Dense(1, activation = 'sigmoid'))\n",
        "c5.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "result5 = leave_one_out(P[0](df), c5)\n",
        "print(\"N = 5 --> Accuracy = \" + str(result5))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}